{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8bfhMZwGNulUHFJbhjp2l"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decission Tree Class"
      ],
      "metadata": {
        "id": "RVGRDUi9Dt8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "def entropy(y):\n",
        "    hist = np.bincount(y)\n",
        "    ps = hist / len(y)\n",
        "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(\n",
        "        self, feature=None, threshold=None, left=None, right=None, *, value=None\n",
        "    ):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.n_feats = n_feats\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # stopping criteria\n",
        "        if (\n",
        "            depth >= self.max_depth\n",
        "            or n_labels == 1\n",
        "            or n_samples < self.min_samples_split\n",
        "        ):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
        "\n",
        "        # greedily select the best split according to information gain\n",
        "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
        "\n",
        "        # grow the children that result from the split\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
        "        return Node(best_feat, best_thresh, left, right)\n",
        "\n",
        "    def _best_criteria(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_thresh = None, None\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(y, X_column, threshold)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_thresh = threshold\n",
        "\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    def _information_gain(self, y, X_column, split_thresh):\n",
        "        # parent loss\n",
        "        parent_entropy = entropy(y)\n",
        "\n",
        "        # generate split\n",
        "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        # compute the weighted avg. of the loss for the children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
        "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
        "\n",
        "        # information gain is difference in loss before vs. after split\n",
        "        ig = parent_entropy - child_entropy\n",
        "        return ig\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        most_common = counter.most_common(1)[0][0]\n",
        "        return most_common\n",
        "\n"
      ],
      "metadata": {
        "id": "TbciRY5HDssI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest Regressor\n"
      ],
      "metadata": {
        "id": "dW-lcfDeHNRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class RandomForestRegressor:\n",
        "    def __init__(self, n_trees=10, max_depth=100, min_samples_split=2, n_feats=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.n_feats = n_feats\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTreeRegressor(\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                max_depth=self.max_depth,\n",
        "                max_features=self.n_feats,\n",
        "            )\n",
        "            X_samp, y_samp = bootstrap_sample(X, y)\n",
        "            tree.fit(X_samp, y_samp)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Get predictions from all trees\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        # Swap axes to align tree predictions (one for each sample) as rows\n",
        "        tree_preds = np.swapaxes(tree_preds, 0, 1)  # Transpose to (n_samples, n_trees)\n",
        "        # For regression, we take the mean of predictions from all trees\n",
        "        y_pred = np.mean(tree_preds, axis=1)  # Take mean across trees for each sample\n",
        "        return y_pred\n",
        "\n",
        "def bootstrap_sample(X, y):\n",
        "    n_samples = X.shape[0]\n",
        "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    return X[idxs], y[idxs]\n",
        "\n",
        "# Custom DecisionTree and related code should be available, but if you are having issues,\n",
        "# you might want to implement it correctly for regression.\n"
      ],
      "metadata": {
        "id": "qNC6-TSqHLEI"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Now, we can use the RandomForestRegressor class you provided.\n",
        "\n",
        "# Create and fit the RandomForestRegressor\n",
        "rf_regressor = RandomForestRegressor(n_trees=10, max_depth=10, min_samples_split=2)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predicting using the trained Random Forest Regressor\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model by calculating the Mean Squared Error (MSE)\n",
        "mse = np.mean((y_pred - y_test) ** 2)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAZ_IrHTKBJA",
        "outputId": "2cad15ab-fcfc-4aa7-a608-48af2c4514b4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 443.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Why Does the Mean Squared Error of a Random Forest Differ from the MSE of Individual Trees?\n",
        "Individual tree predictions can vary quite a bit because each tree is trained on a different random subset of the data (due to bootstrapping). These predictions can be far off from the actual target values and from each other.\n",
        "\n",
        "However, when you take the mean of the predictions from multiple trees in the random forest, the average prediction tends to be more accurate and closer to the true value. This happens because the errors of individual trees tend to cancel each other out, leading to better overall performance."
      ],
      "metadata": {
        "id": "DK0XH-FINuuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "for i, tree in enumerate(rf_regressor.trees):\n",
        "    tree_pred = tree.predict(X_test)  # Get predictions from each individual tree\n",
        "    mse = mean_squared_error(y_test, tree_pred)  # Calculate MSE for the tree\n",
        "    print(f\"Tree {i+1} MSE: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ_lNk70LQgE",
        "outputId": "4e6d5780-7ab0-4cc5-9cba-2fd77e6404a5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree 1 MSE: 1053.47\n",
            "Tree 2 MSE: 1162.34\n",
            "Tree 3 MSE: 1247.05\n",
            "Tree 4 MSE: 1136.89\n",
            "Tree 5 MSE: 1158.63\n",
            "Tree 6 MSE: 1474.86\n",
            "Tree 7 MSE: 1160.98\n",
            "Tree 8 MSE: 1183.25\n",
            "Tree 9 MSE: 1208.67\n",
            "Tree 10 MSE: 1148.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest Classifier\n"
      ],
      "metadata": {
        "id": "nhxZBc-cDS48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "q10oJajQDQDG"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RandomForestClassifier:\n",
        "    def __init__(self, n_trees=10, max_depth=100, min_samples_split=2, n_feats=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.n_feats = n_feats\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                max_depth=self.max_depth,\n",
        "                n_feats=self.n_feats,\n",
        "            )\n",
        "            X_samp, y_samp = bootstrap_sample(X, y)\n",
        "            tree.fit(X_samp, y_samp)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(tree_preds, 0, 1) # This is used get result of each sample one by one by each tree\n",
        "        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "def bootstrap_sample(X, y):\n",
        "    n_samples = X.shape[0]\n",
        "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    return X[idxs], y[idxs]\n",
        "\n",
        "\n",
        "def most_common_label(y):\n",
        "    counter = Counter(y)\n",
        "    most_common = counter.most_common(1)[0][0]\n",
        "    return most_common\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Loading the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Splitting the dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
        "\n",
        "# Create and fit the RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(n_trees=10, max_depth=5, min_samples_split=4)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predicting using the trained Random Forest\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model by comparing predictions with the true labels\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5GMhs3SJR7t",
        "outputId": "4b128b82-3090-410b-a6c4-eda83b70ff23"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, tree in enumerate(rf_clf.trees):\n",
        "    tree_pred = tree.predict(X_test)\n",
        "    accuracy = np.mean(tree_pred == y_test)\n",
        "    print(f\"Tree {i+1} Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1apuc3vvLIdl",
        "outputId": "343a101e-fd9b-48c4-8cfe-9cc8fe4dee42"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree 1 Accuracy: 97.78%\n",
            "Tree 2 Accuracy: 100.00%\n",
            "Tree 3 Accuracy: 100.00%\n",
            "Tree 4 Accuracy: 100.00%\n",
            "Tree 5 Accuracy: 100.00%\n",
            "Tree 6 Accuracy: 93.33%\n",
            "Tree 7 Accuracy: 95.56%\n",
            "Tree 8 Accuracy: 95.56%\n",
            "Tree 9 Accuracy: 97.78%\n",
            "Tree 10 Accuracy: 95.56%\n"
          ]
        }
      ]
    }
  ]
}